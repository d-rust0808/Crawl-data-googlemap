import pandas as pd
import time
import logging
from datetime import datetime
from run_program import get_user_input, build_search_url, scrape_store_details
from function import Scrap_data, opened_link_chroome
from database import DatabaseHandler

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class BatchCrawler:
    """Crawler batch cho nhi·ªÅu t·ª´ kh√≥a v√† ƒë·ªãa ƒëi·ªÉm"""
    
    def __init__(self):
        self.db = DatabaseHandler()
        self.stats = {
            'total_jobs': 0,
            'completed_jobs': 0,
            'total_stores': 0,
            'new_stores': 0,
            'duplicate_stores': 0,
            'start_time': None,
            'end_time': None
        }
    
    def load_jobs_from_txt(self, file_path):
        """Load danh s√°ch job t·ª´ file TXT - format: keyword|location|max_stores"""
        try:
            jobs = []
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            for i, line in enumerate(lines):
                line = line.strip()
                if not line or line.startswith('#'):  # B·ªè qua d√≤ng tr·ªëng v√† comment
                    continue
                
                parts = line.split('|')
                if len(parts) >= 2:
                    job = {
                        'id': len(jobs) + 1,
                        'keyword': parts[0].strip(),
                        'location': parts[1].strip(),
                        'max_stores': int(parts[2].strip()) if len(parts) > 2 and parts[2].strip().isdigit() else 50,
                        'status': 'pending'
                    }
                    jobs.append(job)
            
            logger.info(f"‚úÖ ƒê√£ load {len(jobs)} jobs t·ª´ file TXT")
            return jobs
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói load file TXT: {e}")
            return []
    
    def run_batch_crawl(self, jobs):
        """Ch·∫°y batch crawl cho t·∫•t c·∫£ jobs"""
        self.stats['total_jobs'] = len(jobs)
        self.stats['start_time'] = datetime.now()
        
        logger.info(f"üöÄ B·∫Øt ƒë·∫ßu batch crawl {len(jobs)} jobs...")
        
        # T·∫°o session ID duy nh·∫•t cho batch n√†y
        batch_session = f"batch_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        driver = None
        try:
            for i, job in enumerate(jobs):
                try:
                    logger.info(f"\nüìã === JOB {i+1}/{len(jobs)}: '{job['keyword']}' t·∫°i '{job['location']}' ===")
                    
                    # T·∫°o URL
                    search_url = build_search_url(job['keyword'], job['location'])
                    logger.info(f"üåê URL: {search_url}")
                    
                    # Kh·ªüi t·∫°o driver n·∫øu ch∆∞a c√≥
                    if not driver:
                        driver = opened_link_chroome(search_url)
                    else:
                        driver.get(search_url)
                    
                    # Scrape danh s√°ch c·ª≠a h√†ng
                    logger.info("üìã ƒêang scrape danh s√°ch c·ª≠a h√†ng...")
                    df = Scrap_data(driver)
                    
                    if df.empty:
                        logger.warning(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y c·ª≠a h√†ng n√†o cho '{job['keyword']}' t·∫°i '{job['location']}'")
                        job['status'] = 'no_results'
                        continue
                    
                    logger.info(f"‚úÖ T√¨m th·∫•y {len(df)} c·ª≠a h√†ng")
                    
                    # Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng
                    if job['max_stores'] > 0 and len(df) > job['max_stores']:
                        df = df.head(job['max_stores'])
                        logger.info(f"üî¢ Gi·ªõi h·∫°n: {job['max_stores']} c·ª≠a h√†ng")
                    
                    # X·ª≠ l√Ω t·ª´ng c·ª≠a h√†ng
                    job_new_stores = 0
                    job_duplicate_stores = 0
                    
                    for index, row in df.iterrows():
                        try:
                            logger.info(f"üìù ƒêang x·ª≠ l√Ω c·ª≠a h√†ng {index+1}/{len(df)}: {row['nama'][:30]}...")
                            
                            # Scrape chi ti·∫øt
                            try:
                                details = scrape_store_details(driver, row['link'])
                            except Exception as scrape_error:
                                logger.warning(f"‚ö†Ô∏è L·ªói scrape chi ti·∫øt: {scrape_error}")
                                details = {
                                    'phone': 'Error',
                                    'address': 'Error',
                                    'website': 'Error',
                                    'plus_code': 'Error'
                                }
                            
                            # T·∫°o d·ªØ li·ªáu c·ª≠a h√†ng
                            store_data = {
                                'id': row['id'],
                                'nama': row['nama'],
                                'rating': row['rating'],
                                'link': row['link'],
                                'phone': details['phone'],
                                'address': details['address'],
                                'website': details['website'],
                                'plus_code': details['plus_code'],
                                'search_keyword': job['keyword'],
                                'search_location': job['location'],
                                'crawl_session': batch_session
                            }
                            
                            # L∆∞u v√†o database
                            try:
                                success = self.db.insert_store(store_data)
                                if success:
                                    job_new_stores += 1
                                    self.stats['new_stores'] += 1
                                    logger.info(f"‚úÖ C·ª≠a h√†ng m·ªõi: {row['nama'][:30]}...")
                                else:
                                    job_duplicate_stores += 1
                                    self.stats['duplicate_stores'] += 1
                                    logger.info(f"‚è≠Ô∏è C·ª≠a h√†ng tr√πng l·∫∑p: {row['nama'][:30]}...")
                            except Exception as db_error:
                                logger.warning(f"‚ö†Ô∏è L·ªói l∆∞u database: {db_error}")
                            
                            # Ngh·ªâ m·ªôt ch√∫t
                            time.sleep(1)
                            
                        except KeyboardInterrupt:
                            logger.info("‚èπÔ∏è Ng∆∞·ªùi d√πng d·ª´ng ch∆∞∆°ng tr√¨nh")
                            return
                        except Exception as e:
                            logger.warning(f"‚ö†Ô∏è L·ªói x·ª≠ l√Ω c·ª≠a h√†ng: {e}")
                            continue
                    
                    # C·∫≠p nh·∫≠t k·∫øt qu·∫£ job
                    job['status'] = 'completed'
                    job['stores_found'] = len(df)
                    job['new_stores'] = job_new_stores
                    job['duplicate_stores'] = job_duplicate_stores
                    
                    self.stats['completed_jobs'] += 1
                    self.stats['total_stores'] += len(df)
                    
                    logger.info(f"‚úÖ Ho√†n th√†nh job {i+1}: {job_new_stores} m·ªõi, {job_duplicate_stores} tr√πng l·∫∑p")
                    
                    # Ngh·ªâ gi·ªØa c√°c jobs
                    if i < len(jobs) - 1:
                        logger.info("‚è∏Ô∏è Ngh·ªâ 3 gi√¢y tr∆∞·ªõc job ti·∫øp theo...")
                        time.sleep(3)
                    
                except Exception as job_error:
                    logger.error(f"‚ùå L·ªói job {i+1}: {job_error}")
                    job['status'] = 'error'
                    job['error'] = str(job_error)
                    continue
        
        except Exception as e:
            logger.error(f"‚ùå L·ªói nghi√™m tr·ªçng trong batch crawl: {e}")
        
        finally:
            if driver:
                try:
                    driver.quit()
                    logger.info("üîö ƒê√£ ƒë√≥ng driver")
                except:
                    pass
        
        # K·∫øt th√∫c
        self.stats['end_time'] = datetime.now()
        self._print_final_stats()
        
        return jobs
    
    def _print_final_stats(self):
        """In th·ªëng k√™ cu·ªëi c√πng"""
        duration = self.stats['end_time'] - self.stats['start_time']
        
        print(f"\nüéâ === K·∫æT QU·∫¢ BATCH CRAWL ===")
        print(f"‚è±Ô∏è Th·ªùi gian: {duration}")
        print(f"üìã Jobs ho√†n th√†nh: {self.stats['completed_jobs']}/{self.stats['total_jobs']}")
        print(f"üè™ T·ªïng c·ª≠a h√†ng t√¨m th·∫•y: {self.stats['total_stores']}")
        print(f"üÜï C·ª≠a h√†ng m·ªõi: {self.stats['new_stores']}")
        print(f"üîÑ C·ª≠a h√†ng tr√πng l·∫∑p: {self.stats['duplicate_stores']}")
        
        # Th·ªëng k√™ database
        total_in_db = self.db.get_store_count()
        print(f"üóÑÔ∏è T·ªïng c·ª≠a h√†ng trong database: {total_in_db}")

def main():
    """H√†m main cho batch crawler"""
    crawler = BatchCrawler()
    
    print("üîç === BATCH CRAWLER ===")
    print("Ch·ªçn file jobs:")
    print("1. list_jobs.txt (m·∫∑c ƒë·ªãnh)")
    print("2. Nh·∫≠p ƒë∆∞·ªùng d·∫´n file kh√°c")
    
    choice = input("L·ª±a ch·ªçn (1/2): ").strip()
    
    if choice == "1":
        file_path = "list_jobs.txt"
    elif choice == "2":
        file_path = input("üìÅ ƒê∆∞·ªùng d·∫´n file TXT: ").strip()
    else:
        print("‚ùå L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá")
        return
    
    # Load jobs
    jobs = crawler.load_jobs_from_txt(file_path)
    
    if not jobs:
        print("‚ùå Kh√¥ng c√≥ job n√†o ƒë·ªÉ crawl")
        return
    
    # Hi·ªÉn th·ªã danh s√°ch jobs
    print(f"\nüìã Danh s√°ch {len(jobs)} jobs:")
    for job in jobs:
        print(f"  {job['id']}. '{job['keyword']}' t·∫°i '{job['location']}' (t·ªëi ƒëa {job['max_stores']})")
    
    # X√°c nh·∫≠n
    confirm = input(f"\n‚ùì B·∫Øt ƒë·∫ßu crawl {len(jobs)} jobs? (y/n): ").strip().lower()
    if confirm != 'y':
        print("üëã H·ªßy b·ªè!")
        return
    
    # Ch·∫°y batch crawl
    results = crawler.run_batch_crawl(jobs)
    
    print(f"\nüéâ Ho√†n th√†nh batch crawl!")

if __name__ == "__main__":
    main()