import pandas as pd
import time
import logging
import threading
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from run_program import get_user_input, build_search_url, scrape_store_details
from function import Scrap_data, opened_link_chroome
from database import DatabaseHandler
from config import MAX_WORKERS, THREAD_DELAY

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class BatchCrawler:
    """Crawler batch cho nhi·ªÅu t·ª´ kh√≥a v√† ƒë·ªãa ƒëi·ªÉm - H·ªó tr·ª£ ƒëa lu·ªìng"""
    
    def __init__(self):
        self.db = DatabaseHandler()
        self.stats = {
            'total_jobs': 0,
            'completed_jobs': 0,
            'total_stores': 0,
            'new_stores': 0,
            'duplicate_stores': 0,
            'cached_stores': 0,  # Th√™m th·ªëng k√™ cache
            'start_time': None,
            'end_time': None
        }
        self.stats_lock = threading.Lock()  # Thread lock cho stats
        
        # Cache RAM ƒë·ªÉ tr√°nh scrape l·∫°i c·ª≠a h√†ng ƒë√£ t√¨m th·∫•y
        self.store_cache = {}  # {store_link: store_data}
        self.cache_lock = threading.Lock()  # Thread lock cho cache
    
    def get_cached_store(self, store_link):
        """L·∫•y c·ª≠a h√†ng t·ª´ cache n·∫øu c√≥"""
        with self.cache_lock:
            return self.store_cache.get(store_link)
    
    def cache_store(self, store_link, store_data):
        """L∆∞u c·ª≠a h√†ng v√†o cache"""
        with self.cache_lock:
            self.store_cache[store_link] = store_data
    
    def load_jobs_from_txt(self, file_path):
        """Load danh s√°ch job t·ª´ file TXT - format: keyword|location|max_stores"""
        try:
            jobs = []
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            for i, line in enumerate(lines):
                line = line.strip()
                if not line or line.startswith('#'):  # B·ªè qua d√≤ng tr·ªëng v√† comment
                    continue
                
                parts = line.split('|')
                if len(parts) >= 2:
                    job = {
                        'id': len(jobs) + 1,
                        'keyword': parts[0].strip(),
                        'location': parts[1].strip(),
                        'max_stores': int(parts[2].strip()) if len(parts) > 2 and parts[2].strip().isdigit() else 50,
                        'status': 'pending'
                    }
                    jobs.append(job)
            
            logger.info(f"‚úÖ ƒê√£ load {len(jobs)} jobs t·ª´ file TXT")
            return jobs
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói load file TXT: {e}")
            return []
    
    def process_single_job(self, job, batch_session):
        """X·ª≠ l√Ω m·ªôt job ƒë∆°n l·∫ª - Thread Safe"""
        try:
            logger.info(f"üìã === JOB {job['id']}: '{job['keyword']}' t·∫°i '{job['location']}' ===")
            
            # T·∫°o URL
            search_url = build_search_url(job['keyword'], job['location'])
            logger.info(f"üåê URL: {search_url}")
            
            # Kh·ªüi t·∫°o driver - th·ª≠ kh√¥ng proxy tr∆∞·ªõc
            try:
                logger.info("üîÑ Th·ª≠ kh·ªüi t·∫°o driver kh√¥ng proxy tr∆∞·ªõc...")
                driver = opened_link_chroome(search_url, use_proxy=False)
            except Exception as driver_error:
                logger.warning(f"‚ö†Ô∏è L·ªói kh·ªüi t·∫°o driver kh√¥ng proxy: {driver_error}")
                logger.info("üîÑ Th·ª≠ kh·ªüi t·∫°o driver v·ªõi proxy...")
                try:
                    driver = opened_link_chroome(search_url, use_proxy=True)
                except Exception as proxy_error:
                    logger.error(f"‚ùå L·ªói kh·ªüi t·∫°o driver v·ªõi proxy: {proxy_error}")
                    raise
            
            try:
                # Scrape danh s√°ch c·ª≠a h√†ng
                logger.info("üìã ƒêang scrape danh s√°ch c·ª≠a h√†ng...")
                df = Scrap_data(driver)
                
                if df.empty:
                    logger.warning(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y c·ª≠a h√†ng n√†o cho '{job['keyword']}' t·∫°i '{job['location']}'")
                    job['status'] = 'no_results'
                    return job
                
                logger.info(f"‚úÖ T√¨m th·∫•y {len(df)} c·ª≠a h√†ng")
                
                # Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng
                if job['max_stores'] > 0 and len(df) > job['max_stores']:
                    df = df.head(job['max_stores'])
                    logger.info(f"üî¢ Gi·ªõi h·∫°n: {job['max_stores']} c·ª≠a h√†ng")
                
                # X·ª≠ l√Ω t·ª´ng c·ª≠a h√†ng
                job_new_stores = 0
                job_duplicate_stores = 0
                
                for index, row in df.iterrows():
                    try:
                        logger.info(f"üìù ƒêang x·ª≠ l√Ω c·ª≠a h√†ng {index+1}/{len(df)}: {row['nama'][:30]}...")
                        
                        # Ki·ªÉm tra cache tr∆∞·ªõc
                        store_link = row['link']
                        cached_store = self.get_cached_store(store_link)
                        
                        if cached_store:
                            logger.info(f"üíæ S·ª≠ d·ª•ng cache cho: {row['nama'][:30]}...")
                            details = {
                                'phone': cached_store.get('phone', 'Not Found'),
                                'address': cached_store.get('address', 'Not Found'),
                                'website': cached_store.get('website', 'Not Found'),
                                'plus_code': cached_store.get('plus_code', 'Not Found')
                            }
                            # C·∫≠p nh·∫≠t th·ªëng k√™ cache
                            with self.stats_lock:
                                self.stats['cached_stores'] += 1
                        else:
                            # Scrape chi ti·∫øt n·∫øu ch∆∞a c√≥ trong cache
                            try:
                                details = scrape_store_details(driver, row['link'])
                            except Exception as scrape_error:
                                logger.warning(f"‚ö†Ô∏è L·ªói scrape chi ti·∫øt: {scrape_error}")
                                details = {
                                    'phone': 'Error',
                                    'address': 'Error',
                                    'website': 'Error',
                                    'plus_code': 'Error'
                                }
                        
                        # T·∫°o d·ªØ li·ªáu c·ª≠a h√†ng
                        store_data = {
                            'id': row['id'],
                            'nama': row['nama'],
                            'rating': row['rating'],
                            'link': row['link'],
                            'phone': details['phone'],
                            'address': details['address'],
                            'website': details['website'],
                            'plus_code': details['plus_code'],
                            'search_keyword': job['keyword'],
                            'search_location': job['location'],
                            'crawl_session': batch_session
                        }
                        
                        # L∆∞u v√†o cache n·∫øu ch∆∞a c√≥
                        if not cached_store:
                            self.cache_store(store_link, {
                                'phone': details['phone'],
                                'address': details['address'],
                                'website': details['website'],
                                'plus_code': details['plus_code']
                            })
                        
                        # L∆∞u v√†o database
                        try:
                            logger.info(f"üíæ ƒêang l∆∞u c·ª≠a h√†ng v√†o database: {row['nama'][:30]}...")
                            logger.info(f"üîç DEBUG store_data keys: {list(store_data.keys())}")
                            logger.info(f"üîç DEBUG store_data phone: '{store_data.get('phone', 'N/A')}'")
                            logger.info(f"üîç DEBUG store_data nama: '{store_data.get('nama', 'N/A')}'")
                            
                            success = self.db.insert_store(store_data)
                            
                            logger.info(f"üîç DEBUG insert_store returned: {success}")
                            
                            if success:
                                job_new_stores += 1
                                with self.stats_lock:
                                    self.stats['new_stores'] += 1
                                logger.info(f"‚úÖ C·ª≠a h√†ng m·ªõi: {row['nama'][:30]}...")
                            else:
                                job_duplicate_stores += 1
                                with self.stats_lock:
                                    self.stats['duplicate_stores'] += 1
                                logger.info(f"‚è≠Ô∏è C·ª≠a h√†ng b·ªã skip (tr√πng s·ªë ƒëi·ªán tho·∫°i ho·∫∑c kh√¥ng c√≥ s·ªë ƒëi·ªán tho·∫°i): {row['nama'][:30]}...")
                        except Exception as db_error:
                            logger.error(f"‚ùå L·ªói l∆∞u database: {db_error}")
                            logger.error(f"   Store data: {store_data}")
                            import traceback
                            logger.error(f"   Traceback: {traceback.format_exc()}")
                        
                        # Ngh·ªâ m·ªôt ch√∫t gi·ªØa c√°c c·ª≠a h√†ng ƒë·ªÉ tr√°nh b·ªã ch·∫∑n
                        time.sleep(1.0)  # TƒÉng l√™n 1s ƒë·ªÉ tr√°nh b·ªã ch·∫∑n
                        
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è L·ªói x·ª≠ l√Ω c·ª≠a h√†ng: {e}")
                        continue
                
                # C·∫≠p nh·∫≠t k·∫øt qu·∫£ job
                job['status'] = 'completed'
                job['stores_found'] = len(df)
                job['new_stores'] = job_new_stores
                job['duplicate_stores'] = job_duplicate_stores
                
                with self.stats_lock:
                    self.stats['completed_jobs'] += 1
                    self.stats['total_stores'] += len(df)
                
                logger.info(f"‚úÖ Ho√†n th√†nh job {job['id']}: {job_new_stores} m·ªõi, {job_duplicate_stores} tr√πng l·∫∑p")
                
            finally:
                # ƒê√≥ng driver
                try:
                    driver.quit()
                    logger.info(f"üîö ƒê√£ ƒë√≥ng driver cho job {job['id']}")
                except:
                    pass
            
            return job
            
        except Exception as job_error:
            logger.error(f"‚ùå L·ªói job {job['id']}: {job_error}")
            job['status'] = 'error'
            job['error'] = str(job_error)
            return job
    
    def run_batch_crawl(self, jobs):
        """Ch·∫°y batch crawl cho t·∫•t c·∫£ jobs - H·ªó tr·ª£ ƒëa lu·ªìng"""
        self.stats['total_jobs'] = len(jobs)
        self.stats['start_time'] = datetime.now()
        
        logger.info(f"üöÄ B·∫Øt ƒë·∫ßu batch crawl {len(jobs)} jobs v·ªõi {MAX_WORKERS} lu·ªìng...")
        
        # T·∫°o session ID duy nh·∫•t cho batch n√†y
        batch_session = f"batch_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        try:
            # S·ª≠ d·ª•ng ThreadPoolExecutor ƒë·ªÉ ch·∫°y ƒëa lu·ªìng
            with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
                # Submit t·∫•t c·∫£ jobs
                future_to_job = {
                    executor.submit(self.process_single_job, job, batch_session): job 
                    for job in jobs
                }
                
                # X·ª≠ l√Ω k·∫øt qu·∫£ khi ho√†n th√†nh
                for future in as_completed(future_to_job):
                    job = future_to_job[future]
                    try:
                        result = future.result()
                        logger.info(f"‚úÖ Job {result['id']} ho√†n th√†nh: {result['status']}")
                        
                        # Th√™m delay gi·ªØa c√°c job ƒë·ªÉ tr√°nh b·ªã ch·∫∑n
                        if MAX_WORKERS == 1:  # Ch·ªâ delay khi ch·∫°y 1 lu·ªìng
                            logger.info(f"‚è≥ Ch·ªù {THREAD_DELAY}s tr∆∞·ªõc job ti·∫øp theo...")
                            time.sleep(THREAD_DELAY)
                            
                    except Exception as exc:
                        logger.error(f"‚ùå Job {job['id']} l·ªói: {exc}")
                        job['status'] = 'error'
                        job['error'] = str(exc)
        
        except KeyboardInterrupt:
            logger.info("‚èπÔ∏è Ng∆∞·ªùi d√πng d·ª´ng ch∆∞∆°ng tr√¨nh")
            return jobs
        except Exception as e:
            logger.error(f"‚ùå L·ªói nghi√™m tr·ªçng trong batch crawl: {e}")
        
        # K·∫øt th√∫c
        self.stats['end_time'] = datetime.now()
        self._print_final_stats()
        
        return jobs
    
    def _print_final_stats(self):
        """In th·ªëng k√™ cu·ªëi c√πng"""
        duration = self.stats['end_time'] - self.stats['start_time']
        
        print(f"\nüéâ === K·∫æT QU·∫¢ BATCH CRAWL ===")
        print(f"‚è±Ô∏è Th·ªùi gian: {duration}")
        print(f"üìã Jobs ho√†n th√†nh: {self.stats['completed_jobs']}/{self.stats['total_jobs']}")
        print(f"üè™ T·ªïng c·ª≠a h√†ng t√¨m th·∫•y: {self.stats['total_stores']}")
        print(f"üÜï C·ª≠a h√†ng m·ªõi: {self.stats['new_stores']}")
        print(f"üîÑ C·ª≠a h√†ng tr√πng l·∫∑p: {self.stats['duplicate_stores']}")
        print(f"üíæ C·ª≠a h√†ng t·ª´ cache: {self.stats['cached_stores']}")
        print(f"üìä Cache size: {len(self.store_cache)} c·ª≠a h√†ng")
        
        # Th·ªëng k√™ database
        total_in_db = self.db.get_store_count()
        print(f"üóÑÔ∏è T·ªïng c·ª≠a h√†ng trong database: {total_in_db}")

def main():
    """H√†m main cho batch crawler"""
    crawler = BatchCrawler()
    
    print("üîç === BATCH CRAWLER ===")
    print("üöÄ T·ª± ƒë·ªông ch·∫°y v·ªõi list_jobs.txt...")
    
    # Load jobs t·ª´ file m·∫∑c ƒë·ªãnh
    file_path = "list_jobs.txt"
    jobs = crawler.load_jobs_from_txt(file_path)
    
    if not jobs:
        print("‚ùå Kh√¥ng c√≥ job n√†o ƒë·ªÉ crawl")
        return
    
    # Hi·ªÉn th·ªã danh s√°ch jobs
    print(f"\nüìã Danh s√°ch {len(jobs)} jobs:")
    for job in jobs:
        print(f"  {job['id']}. '{job['keyword']}' t·∫°i '{job['location']}' (t·ªëi ƒëa {job['max_stores']})")
    
    print(f"\nüöÄ B·∫Øt ƒë·∫ßu crawl {len(jobs)} jobs...")
    
    # Ch·∫°y batch crawl
    results = crawler.run_batch_crawl(jobs)
    
    print(f"\nüéâ Ho√†n th√†nh batch crawl!")

if __name__ == "__main__":
    main()