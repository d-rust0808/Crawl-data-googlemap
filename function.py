from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver import ActionChains
from selenium.webdriver.common.actions.wheel_input import ScrollOrigin
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
import time
import datetime
import numpy as np
import pandas as pd
from bs4 import BeautifulSoup
import logging

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def opened_link_chroome(url_search):
    options = webdriver.ChromeOptions()
    options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('--disable-gpu')
    options.add_argument('--window-size=1920,1080')
    options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')
    
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=options)
    driver.set_window_size(1920, 1080)
    
    logger.info(f"üåê ƒêang m·ªü URL: {url_search}")
    driver.get(url_search)
    
    # Ch·ªù trang load
    WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.TAG_NAME, "body"))
    )
    time.sleep(3)
    
    return driver
def Scrap_data(driver):
    logger.info("üîç B·∫Øt ƒë·∫ßu scraping data t·ª´ Google Maps...")
    
    # C√°c selectors m·ªõi cho Google Maps hi·ªán t·∫°i
    store_selectors = [
        "a.hfpxzc",  # Link c·ª≠a h√†ng ch√≠nh
        "a[aria-label*='¬∑']",  # C·ª≠a h√†ng c√≥ d·∫•u ¬∑
        "div[role='main'] a[jsaction]",  # Link trong main area
        "div[data-value] a[href*='/place/']",  # Link ƒë·∫øn place
        "a[jslog*='track:click']",  # C√≥ jslog track click
        "div[class*='Nv2PK'] a",  # Link trong container Nv2PK
        "div[class*='Q2HXcd'] a",  # Link trong container Q2HXcd
    ]
    
    # T√¨m elements ƒë·ªÉ scroll
    scroll_elements = []
    for selector in store_selectors:
        try:
            elements = driver.find_elements(By.CSS_SELECTOR, selector)
            if elements:
                scroll_elements.extend(elements)
                logger.info(f"‚úÖ T√¨m th·∫•y {len(elements)} elements v·ªõi selector: {selector}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è L·ªói v·ªõi selector {selector}: {e}")
    
    if not scroll_elements:
        logger.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y elements ƒë·ªÉ scroll, th·ª≠ c√°ch kh√°c...")
        # Fallback: t√¨m t·∫•t c·∫£ th·∫ª a
        scroll_elements = driver.find_elements(By.TAG_NAME, "a")
        logger.info(f"üìã T√¨m th·∫•y {len(scroll_elements)} th·∫ª <a>")
    
    action = ActionChains(driver)
    scroll_count = 0
    max_scrolls = 10
    
    while scroll_count < max_scrolls:
        try:
            logger.info(f"üìú Scroll l·∫ßn {scroll_count + 1}/{max_scrolls}")
            
            # Scroll xu·ªëng
            if scroll_elements:
                last_element = scroll_elements[-1]
                Scroll_origin = ScrollOrigin.from_element(last_element)
                action.scroll_from_origin(Scroll_origin, 0, 1000).perform()
            else:
                # Fallback scroll
                driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            
            time.sleep(2)
            
            # Ki·ªÉm tra xem c√≥ th√™m elements m·ªõi kh√¥ng
            new_elements = []
            for selector in store_selectors:
                try:
                    elements = driver.find_elements(By.CSS_SELECTOR, selector)
                    new_elements.extend(elements)
                except:
                    continue
            
            if len(new_elements) > len(scroll_elements):
                scroll_elements = new_elements
                logger.info(f"üîÑ T√¨m th·∫•y th√™m elements, t·ªïng: {len(scroll_elements)}")
            else:
                logger.info("‚úÖ Kh√¥ng c√≥ th√™m elements m·ªõi, d·ª´ng scroll")
                break
                
            scroll_count += 1
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è L·ªói khi scroll: {e}")
            break
    
    # Parse HTML v√† extract data
    content = driver.page_source
    data = BeautifulSoup(content, 'html.parser')
    
    logger.info("üìã B·∫Øt ƒë·∫ßu parse data...")
    res = []
    
    # C√°c selectors ƒë·ªÉ t√¨m th√¥ng tin c·ª≠a h√†ng
    store_containers = [
        "div[class*='Nv2PK']",
        "div[class*='Q2HXcd']", 
        "div[class*='THOPZb']",
        "div[role='main'] > div",
        "div[data-value]"
    ]
    
    for container_selector in store_containers:
        try:
            containers = data.find_all('div', class_=lambda x: x and any(cls in x for cls in ['Nv2PK', 'Q2HXcd', 'THOPZb']))
            logger.info(f"üîç T√¨m th·∫•y {len(containers)} containers v·ªõi selector: {container_selector}")
            
            for i, area in enumerate(containers):
                try:
                    # Kh·ªüi t·∫°o bi·∫øn link tr∆∞·ªõc
                    link = "Link Not Found"
                    
                    # T√¨m link tr∆∞·ªõc
                    link_selectors = [
                        "a[href*='/maps/place/']",
                        "a[href*='google.com/maps']",
                        "a[data-value]",
                        "a[jsaction*='pane']"
                    ]
                    
                    for link_selector in link_selectors:
                        try:
                            link_elem = area.select_one(link_selector)
                            if link_elem and link_elem.get('href'):
                                link = link_elem.get('href')
                                break
                        except:
                            continue
                    
                    # T·∫°o ID duy nh·∫•t d·ª±a tr√™n link thay v√¨ timestamp
                    import hashlib
                    if link != "Link Not Found":
                        store_id = hashlib.md5(link.encode()).hexdigest()[:16]
                    else:
                        # Fallback n·∫øu kh√¥ng c√≥ link
                        current_datetime = datetime.datetime.now()
                        merge_date = current_datetime.strftime("%Y%m%d%H%M%S%f")
                        store_id = f"{merge_date}{i+1}"
                    
                    # T√¨m t√™n c·ª≠a h√†ng
                    nama = "Nama Not Found"
                    name_selectors = [
                        "div[class*='qBF1Pd']",
                        "div[class*='fontHeadlineSmall']", 
                        "h1", "h2", "h3",
                        "span[class*='fontHeadlineSmall']",
                        "div[class*='fontBodyMedium']"
                    ]
                    
                    for name_selector in name_selectors:
                        try:
                            name_elem = area.select_one(name_selector)
                            if name_elem and name_elem.get_text().strip():
                                nama = name_elem.get_text().strip()
                                break
                        except:
                            continue
                    
                    # T√¨m rating
                    rating = "Rating Not Found"
                    rating_selectors = [
                        "span[class*='MW4etd']",
                        "span[class*='fontBodyMedium']",
                        "div[class*='fontBodyMedium']",
                        "span[class*='rating']"
                    ]
                    
                    for rating_selector in rating_selectors:
                        try:
                            rating_elem = area.select_one(rating_selector)
                            if rating_elem and rating_elem.get_text().strip():
                                rating_text = rating_elem.get_text().strip()
                                if any(char.isdigit() for char in rating_text):
                                    rating = rating_text
                                    break
                        except:
                            continue
                    
                    
                    # Ch·ªâ th√™m n·∫øu c√≥ √≠t nh·∫•t t√™n ho·∫∑c link
                    if nama != "Nama Not Found" or link != "Link Not Found":
                        res.append({
                            'id': store_id, 
                            'nama': nama, 
                            'rating': rating, 
                            'link': link
                        })
                        logger.info(f"‚úÖ T√¨m th·∫•y c·ª≠a h√†ng: {nama[:50]}...")
                        
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è L·ªói khi parse c·ª≠a h√†ng {i+1}: {e}")
                    continue
                    
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è L·ªói v·ªõi container selector {container_selector}: {e}")
            continue
    
    # Lo·∫°i b·ªè duplicate
    unique_res = []
    seen_links = set()
    
    for item in res:
        if item['link'] not in seen_links:
            unique_res.append(item)
            seen_links.add(item['link'])
    
    logger.info(f"üéâ Ho√†n th√†nh scraping! T√¨m th·∫•y {len(unique_res)} c·ª≠a h√†ng")
    
    df = pd.DataFrame(unique_res)
    return df