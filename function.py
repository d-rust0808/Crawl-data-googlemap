from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver import ActionChains
from selenium.webdriver.common.actions.wheel_input import ScrollOrigin
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.proxy import Proxy, ProxyType
import time
import datetime
import numpy as np
import pandas as pd
from bs4 import BeautifulSoup
import logging
import threading
import os
import zipfile
import tempfile
from config import PROXY_HOST, PROXY_PORT, PROXY_USERNAME, PROXY_PASSWORD, PROXY_RETRY_COUNT
from proxy_manager import proxy_manager

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def create_proxy_auth_extension(proxy_auth):
    """T·∫°o Chrome extension ƒë·ªÉ x·ª≠ l√Ω proxy authentication"""
    if not proxy_auth:
        return None
    
    username, password = proxy_auth.split(':')
    
    # T·∫°o manifest.json
    manifest_json = """
    {
        "version": "1.0.0",
        "manifest_version": 2,
        "name": "Chrome Proxy Auth",
        "permissions": [
            "proxy",
            "tabs",
            "unlimitedStorage",
            "storage",
            "<all_urls>",
            "webRequest",
            "webRequestBlocking"
        ],
        "background": {
            "scripts": ["background.js"]
        },
        "minimum_chrome_version":"22.0.0"
    }
    """
    
    # T·∫°o background.js
    background_js = f"""
    var config = {{
        mode: "fixed_servers",
        rules: {{
            singleProxy: {{
                scheme: "http",
                host: "{PROXY_HOST}",
                port: parseInt({PROXY_PORT})
            }},
            bypassList: ["localhost"]
        }}
    }};

    chrome.proxy.settings.set({{value: config, scope: "regular"}}, function() {{}});

    function callbackFn(details) {{
        return {{
            authCredentials: {{
                username: "{username}",
                password: "{password}"
            }}
        }};
    }}

    chrome.webRequest.onAuthRequired.addListener(
        callbackFn,
        {{urls: ["<all_urls>"]}},
        ['blocking']
    );
    """
    
    # T·∫°o extension zip file
    pluginfile = os.path.join(tempfile.gettempdir(), 'proxy_auth_plugin.zip')
    
    with zipfile.ZipFile(pluginfile, 'w') as zp:
        zp.writestr("manifest.json", manifest_json)
        zp.writestr("background.js", background_js)
    
    return pluginfile


def opened_link_chroome(url_search, use_proxy=True, retry_count=0):
    """
    M·ªü Chrome driver v·ªõi proxy support v√† rotation
    """
    options = webdriver.ChromeOptions()
    options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('--disable-gpu')
    options.add_argument('--window-size=1920,1080')
    options.add_argument('--disable-blink-features=AutomationControlled')
    options.add_argument('--disable-extensions')
    options.add_argument('--disable-plugins')
    options.add_argument('--disable-images')
    options.add_argument('--disable-web-security')
    options.add_argument('--allow-running-insecure-content')
    options.add_argument('--disable-features=VizDisplayCompositor')
    options.add_argument('--disable-ipc-flooding-protection')
    options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
    
    # Th√™m stealth options
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option('useAutomationExtension', False)
    options.add_argument("--disable-blink-features=AutomationControlled")
    
    # Th√™m proxy n·∫øu ƒë∆∞·ª£c y√™u c·∫ßu
    current_proxy = None
    if use_proxy:
        current_proxy = proxy_manager.get_current_proxy()
        if current_proxy:
            proxy_string = proxy_manager.get_proxy_string(current_proxy)
            proxy_auth = proxy_manager.get_proxy_auth(current_proxy)
            
            # S·ª≠ d·ª•ng format ƒë√∫ng cho proxy authentication
            options.add_argument(f'--proxy-server=http://{proxy_string}')
            
            # Th√™m proxy authentication extension
            try:
                extension_path = create_proxy_auth_extension(proxy_auth)
                options.add_extension(extension_path)
                logger.info(f"üîí S·ª≠ d·ª•ng proxy: {proxy_string}")
            except Exception as ext_error:
                logger.error(f"‚ùå L·ªói t·∫°o proxy extension: {ext_error}")
                raise Exception(f"‚ùå Kh√¥ng th·ªÉ t·∫°o proxy extension: {ext_error}")
        else:
            raise Exception("‚ùå B·∫ÆT BU·ªòC ph·∫£i c√≥ proxy!")
    
    try:
        # T·∫£i ChromeDriver tr∆∞·ªõc (kh√¥ng qua proxy)
        logger.info("üì• ƒêang t·∫£i ChromeDriver...")
        service = Service(ChromeDriverManager().install())
        
        # T·∫°o driver
        driver = webdriver.Chrome(service=service, options=options)
        driver.set_window_size(1920, 1080)
        
        # Th√™m stealth JavaScript
        driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        driver.execute_script("Object.defineProperty(navigator, 'plugins', {get: () => [1, 2, 3, 4, 5]})")
        driver.execute_script("Object.defineProperty(navigator, 'languages', {get: () => ['en-US', 'en']})")
        driver.execute_script("window.chrome = { runtime: {} }")
        
        logger.info(f"üåê ƒêang m·ªü URL: {url_search}")
        driver.get(url_search)
        
        # Ch·ªù trang load
        WebDriverWait(driver, 15).until(
            EC.presence_of_element_located((By.TAG_NAME, "body"))
        )
        time.sleep(5)  # TƒÉng th·ªùi gian ch·ªù
        
        # Debug: Ki·ªÉm tra title v√† URL
        try:
            title = driver.title
            current_url = driver.current_url
            logger.info(f"üìÑ Page title: {title}")
            logger.info(f"üîó Current URL: {current_url}")
            
            # Ki·ªÉm tra xem c√≥ b·ªã ch·∫∑n kh√¥ng
            if "blocked" in title.lower() or "access denied" in title.lower() or "captcha" in title.lower():
                logger.warning("‚ö†Ô∏è C√≥ th·ªÉ b·ªã ch·∫∑n b·ªüi Google Maps")
        except Exception as debug_error:
            logger.warning(f"‚ö†Ô∏è L·ªói debug: {debug_error}")
        
        # Reset retry count n·∫øu th√†nh c√¥ng
        proxy_manager.reset_retry()
        return driver
        
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è L·ªói kh·ªüi t·∫°o driver: {e}")
        
        # ƒê√°nh d·∫•u proxy fail n·∫øu c√≥
        if current_proxy:
            proxy_manager.mark_proxy_failed(current_proxy)
        
        # Retry logic - th·ª≠ kh√¥ng proxy n·∫øu proxy fail
        if proxy_manager.should_retry():
            proxy_manager.increment_retry()
            delay = proxy_manager.get_retry_delay()
            logger.info(f"üîÑ Retry {proxy_manager.retry_count}/{PROXY_RETRY_COUNT} sau {delay:.1f}s...")
            time.sleep(delay)
            
            # Th·ª≠ l·∫°i v·ªõi proxy n·∫øu c√≤n retry
            if proxy_manager.retry_count < PROXY_RETRY_COUNT:
                return opened_link_chroome(url_search, use_proxy=use_proxy, retry_count=retry_count + 1)
            else:
                # Kh√¥ng cho ph√©p ch·∫°y kh√¥ng proxy
                logger.error(f"‚ùå Proxy fail sau {PROXY_RETRY_COUNT} l·∫ßn th·ª≠ - B·∫ÆT BU·ªòC ph·∫£i d√πng proxy!")
                raise Exception("‚ùå B·∫ÆT BU·ªòC ph·∫£i d√πng proxy!")
        else:
            # Kh√¥ng cho ph√©p ch·∫°y kh√¥ng proxy
            logger.error("‚ùå Proxy fail - B·∫ÆT BU·ªòC ph·∫£i d√πng proxy!")
            raise Exception("‚ùå B·∫ÆT BU·ªòC ph·∫£i d√πng proxy!")
def Scrap_data(driver):
    logger.info("üîç B·∫Øt ƒë·∫ßu scraping data t·ª´ Google Maps...")
    
    # C√°c selectors m·ªõi cho Google Maps hi·ªán t·∫°i
    store_selectors = [
        "a.hfpxzc",  # Link c·ª≠a h√†ng ch√≠nh
        "a[aria-label*='¬∑']",  # C·ª≠a h√†ng c√≥ d·∫•u ¬∑
        "div[role='main'] a[jsaction]",  # Link trong main area
        "div[data-value] a[href*='/place/']",  # Link ƒë·∫øn place
        "a[jslog*='track:click']",  # C√≥ jslog track click
        "div[class*='Nv2PK'] a",  # Link trong container Nv2PK
        "div[class*='Q2HXcd'] a",  # Link trong container Q2HXcd
        "a[href*='/maps/place/']",  # Link tr·ª±c ti·∫øp ƒë·∫øn place
        "div[class*='THOPZb'] a",  # Link trong container THOPZb
        "div[class*='VkpGBb'] a",  # Link trong container VkpGBb
        "a[data-value]",  # Link c√≥ data-value
        "div[jsaction] a",  # Link trong div c√≥ jsaction
    ]
    
    # T√¨m elements ƒë·ªÉ scroll
    scroll_elements = []
    for selector in store_selectors:
        try:
            elements = driver.find_elements(By.CSS_SELECTOR, selector)
            if elements:
                scroll_elements.extend(elements)
                logger.info(f"‚úÖ T√¨m th·∫•y {len(elements)} elements v·ªõi selector: {selector}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è L·ªói v·ªõi selector {selector}: {e}")
    
    if not scroll_elements:
        logger.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y elements ƒë·ªÉ scroll, th·ª≠ c√°ch kh√°c...")
        # Fallback: t√¨m t·∫•t c·∫£ th·∫ª a
        scroll_elements = driver.find_elements(By.TAG_NAME, "a")
        logger.info(f"üìã T√¨m th·∫•y {len(scroll_elements)} th·∫ª <a>")
    
    action = ActionChains(driver)
    scroll_count = 0
    max_scrolls = 10  # Gi·∫£m s·ªë l·∫ßn scroll ƒë·ªÉ tƒÉng t·ªëc ƒë·ªô
    
    while scroll_count < max_scrolls:
        try:
            logger.info(f"üìú Scroll l·∫ßn {scroll_count + 1}/{max_scrolls}")
            
            # Scroll xu·ªëng
            if scroll_elements:
                last_element = scroll_elements[-1]
                Scroll_origin = ScrollOrigin.from_element(last_element)
                action.scroll_from_origin(Scroll_origin, 0, 1000).perform()
            else:
                # Fallback scroll
                driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            
            time.sleep(1)  # Gi·∫£m th·ªùi gian ch·ªù gi·ªØa c√°c l·∫ßn scroll
            
            # Ki·ªÉm tra xem c√≥ th√™m elements m·ªõi kh√¥ng
            new_elements = []
            for selector in store_selectors:
                try:
                    elements = driver.find_elements(By.CSS_SELECTOR, selector)
                    new_elements.extend(elements)
                except:
                    continue
            
            if len(new_elements) > len(scroll_elements):
                scroll_elements = new_elements
                logger.info(f"üîÑ T√¨m th·∫•y th√™m elements, t·ªïng: {len(scroll_elements)}")
            else:
                logger.info("‚úÖ Kh√¥ng c√≥ th√™m elements m·ªõi, d·ª´ng scroll")
                break
                
            scroll_count += 1
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è L·ªói khi scroll: {e}")
            break
    
    # Parse HTML v√† extract data
    content = driver.page_source
    data = BeautifulSoup(content, 'html.parser')
    
    logger.info("üìã B·∫Øt ƒë·∫ßu parse data...")
    res = []
    
    # C√°c selectors ƒë·ªÉ t√¨m th√¥ng tin c·ª≠a h√†ng
    store_containers = [
        "div[class*='Nv2PK']",
        "div[class*='Q2HXcd']", 
        "div[class*='THOPZb']",
        "div[role='main'] > div",
        "div[data-value]"
    ]
    
    for container_selector in store_containers:
        try:
            containers = data.find_all('div', class_=lambda x: x and any(cls in x for cls in ['Nv2PK', 'Q2HXcd', 'THOPZb']))
            logger.info(f"üîç T√¨m th·∫•y {len(containers)} containers v·ªõi selector: {container_selector}")
            
            for i, area in enumerate(containers):
                try:
                    # Kh·ªüi t·∫°o bi·∫øn link tr∆∞·ªõc
                    link = "Link Not Found"
                    
                    # T√¨m link tr∆∞·ªõc
                    link_selectors = [
                        "a[href*='/maps/place/']",
                        "a[href*='google.com/maps']",
                        "a[data-value]",
                        "a[jsaction*='pane']"
                    ]
                    
                    for link_selector in link_selectors:
                        try:
                            link_elem = area.select_one(link_selector)
                            if link_elem and link_elem.get('href'):
                                link = link_elem.get('href')
                                break
                        except:
                            continue
                    
                    # T·∫°o ID duy nh·∫•t d·ª±a tr√™n link thay v√¨ timestamp
                    import hashlib
                    if link != "Link Not Found":
                        store_id = hashlib.md5(link.encode()).hexdigest()[:16]
                    else:
                        # Fallback n·∫øu kh√¥ng c√≥ link
                        current_datetime = datetime.datetime.now()
                        merge_date = current_datetime.strftime("%Y%m%d%H%M%S%f")
                        store_id = f"{merge_date}{i+1}"
                    
                    # T√¨m t√™n c·ª≠a h√†ng
                    nama = "Nama Not Found"
                    name_selectors = [
                        "div[class*='qBF1Pd']",
                        "div[class*='fontHeadlineSmall']", 
                        "h1", "h2", "h3",
                        "span[class*='fontHeadlineSmall']",
                        "div[class*='fontBodyMedium']"
                    ]
                    
                    for name_selector in name_selectors:
                        try:
                            name_elem = area.select_one(name_selector)
                            if name_elem and name_elem.get_text().strip():
                                nama = name_elem.get_text().strip()
                                break
                        except:
                            continue
                    
                    # T√¨m rating
                    rating = "Rating Not Found"
                    rating_selectors = [
                        "span[class*='MW4etd']",
                        "span[class*='fontBodyMedium']",
                        "div[class*='fontBodyMedium']",
                        "span[class*='rating']"
                    ]
                    
                    for rating_selector in rating_selectors:
                        try:
                            rating_elem = area.select_one(rating_selector)
                            if rating_elem and rating_elem.get_text().strip():
                                rating_text = rating_elem.get_text().strip()
                                if any(char.isdigit() for char in rating_text):
                                    rating = rating_text
                                    break
                        except:
                            continue
                    
                    
                    # Ch·ªâ th√™m n·∫øu c√≥ √≠t nh·∫•t t√™n ho·∫∑c link
                    if nama != "Nama Not Found" or link != "Link Not Found":
                        res.append({
                            'id': store_id, 
                            'nama': nama, 
                            'rating': rating, 
                            'link': link
                        })
                        logger.info(f"‚úÖ T√¨m th·∫•y c·ª≠a h√†ng: {nama[:50]}...")
                        
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è L·ªói khi parse c·ª≠a h√†ng {i+1}: {e}")
                    logger.debug(f"   Container HTML: {str(area)[:200]}...")
                    continue
                    
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è L·ªói v·ªõi container selector {container_selector}: {e}")
            continue
    
    # Lo·∫°i b·ªè duplicate d·ª±a tr√™n t√™n c·ª≠a h√†ng
    unique_res = []
    seen_names = set()
    duplicate_count = 0
    
    for item in res:
        # Chu·∫©n h√≥a t√™n ƒë·ªÉ so s√°nh (b·ªè d·∫•u, chuy·ªÉn th√†nh ch·ªØ th∆∞·ªùng)
        import re
        normalized_name = re.sub(r'[^\w\s]', '', item['nama'].lower().strip())
        
        if normalized_name not in seen_names:
            unique_res.append(item)
            seen_names.add(normalized_name)
        else:
            duplicate_count += 1
            logger.debug(f"üîÑ B·ªè qua duplicate: {item['nama'][:30]}... (t√™n ƒë√£ c√≥)")
    
    logger.info(f"üéâ Ho√†n th√†nh scraping! T√¨m th·∫•y {len(res)} c·ª≠a h√†ng, {duplicate_count} duplicate, {len(unique_res)} unique")
    
    df = pd.DataFrame(unique_res)
    return df